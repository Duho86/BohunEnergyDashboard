# app.py
# -*- coding: utf-8 -*-

from __future__ import annotations

from datetime import datetime
from pathlib import Path
import traceback

import pandas as pd
import streamlit as st

from modules import loader, analyzer, feedback, baseline as baseline_mod

# ============================
# ê¸°ë³¸ ì„¤ì •
# ============================

st.set_page_config(
    page_title="ê³µë‹¨ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ Â· ì˜¨ì‹¤ê°€ìŠ¤ ê´€ë¦¬ ëŒ€ì‹œë³´ë“œ",
    layout="wide",
)

st.title("ê³µë‹¨ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ Â· ì˜¨ì‹¤ê°€ìŠ¤ ê´€ë¦¬ ëŒ€ì‹œë³´ë“œ")

DATA_DIR = Path("data")
ENERGY_DIR = DATA_DIR / "energy"
BASELINE_PATH = DATA_DIR / "baseline.json"

def ensure_dirs():
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    loader.ensure_energy_dir(ENERGY_DIR)

# ============================
# ë°ì´í„° ë¡œë”© í—¬í¼
# ============================

def load_all_energy_data(base_dir: Path = ENERGY_DIR):
    ensure_dirs()
    dfs = []
    meta_list = []
    errors = []

    for xlsx_path in sorted(base_dir.glob("*.xlsx")):
        try:
            df_std, year = loader.load_energy_xlsx(xlsx_path)
            dfs.append(df_std)

            stat = xlsx_path.stat()
            meta_list.append({
                "ì—°ë„": year,
                "íŒŒì¼ëª…": xlsx_path.name,
                "ê²½ë¡œ": str(xlsx_path),
                "ì—…ë¡œë“œì‹œê°„": datetime.fromtimestamp(stat.st_mtime).strftime(
                    "%Y-%m-%d %H:%M:%S"
                ),
            })
        except loader.EnergyDataError as e:
            errors.append({"íŒŒì¼ëª…": xlsx_path.name, "ì—ëŸ¬": str(e)})
        except Exception as e:
            errors.append({"íŒŒì¼ëª…": xlsx_path.name, "ì—ëŸ¬": f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}"})

    df_all = pd.concat(dfs, ignore_index=True) if dfs else None
    return df_all, meta_list, errors


# ======================================
# ì›ë³¸ U/V/W ë¡œë”© í•¨ìˆ˜ (ì—ë„ˆì§€ ë¶„ì„ìš©)
# ======================================

def load_raw_year_data(year: int):
    for p in ENERGY_DIR.glob("*.xlsx"):
        if str(year) in p.name:
            return loader.load_energy_raw_for_analysis(p)
    return None


# ============================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ============================

if "processed_uploads" not in st.session_state:
    st.session_state["processed_uploads"] = set()

ensure_dirs()

baseline_records = baseline_mod.load_baseline_records(BASELINE_PATH)
baseline_map = baseline_mod.get_baseline_map(baseline_records)

# ============================
# í™”ë©´ íƒ­ êµ¬ì„±
# ============================

tab_dashboard, tab_baseline, tab_debug = st.tabs(
    ["ğŸ“Š ëŒ€ì‹œë³´ë“œ", "âš™ï¸ ê¸°ì¤€ë°°ì¶œëŸ‰ ê´€ë¦¬", "ğŸ”§ ë””ë²„ê·¸/ì§„ë‹¨"]
)

# ============================================================
# ğŸ“Š 1) ëŒ€ì‹œë³´ë“œ íƒ­
# ============================================================

with tab_dashboard:

    # -----------------------------
    # ğŸ”§ ì§„í–‰ì‚¬í•­ í‘œì‹œ (ìš”ì²­ì‚¬í•­ ë°˜ì˜)
    # -----------------------------
    with st.expander("ğŸ› ï¸ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ê¸°ëŠ¥ ë°˜ì˜ í˜„í™©"):
        st.markdown("""
        # ğŸ”§ ê¸°ëŠ¥ ë°˜ì˜ í˜„í™©

        ## 1. ê¸°ì¡´ ê¸°ëŠ¥ ë³€ê²½
        - ê¸°ì¡´ **ì „ë§ë¶„ì„ ì„¹ì…˜ ì „ì²´ ì‚­ì œ**
        - ê¸°ì¡´ **í”¼ë“œë°± ì„¹ì…˜ ì¼ë¶€ ì‚­ì œ**
        - ê³µë‹¨ ì „ì²´ ë¶„ì„Â·ì½”ë©˜íŠ¸ëŠ” ìœ ì§€ë¨

        ## 2. ì‹ ê·œ ê¸°ëŠ¥ â€” ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ ë¶„ì„
        - ê³µë‹¨ ì „ì²´ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰(U)
        - ë©´ì ë‹¹ ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œëŸ‰(V)
        - 3ê°œë…„ í‰ê·  ëŒ€ë¹„ ì¦ê°ë¥ 
        - ì‹œì„¤êµ°ë³„(Wì—´ í‰ê· ) ë¶„ì„
        - ì†Œì†ê¸°êµ¬ë³„ ì—ë„ˆì§€ ë¶„ì„ í‘œ

        ## 3. ì‹ ê·œ í”¼ë“œë°± â€” ì—ë„ˆì§€ ê¸°ì¤€
        - ê³µë‹¨ ì „ì²´: í˜„ì¬ì›” / ëª©í‘œë‹¬ì„± ê°ì¶•ë¥ 
        - ì†Œì†ê¸°êµ¬ë³„: ë¶„í¬ìˆœìœ„ / ì¦ê°€ìœ¨ / í‰ê·  ëŒ€ë¹„ / ê¶Œì¥ê°ì¶•ëŸ‰ / ì‚¬ìœ ì œì¶œ(O/X)

        ## 4. ê¸°íƒ€
        - ê¸°ê´€ ì¶œë ¥ ìˆœì„œ ê³ ì •
        - í‘œ ì „ì²´í­ ë°°ì¹˜
        """)

    # ------------------------------
    # íŒŒì¼ ì—…ë¡œë“œ
    # ------------------------------
    st.markdown("### ì›”ë³„ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ íŒŒì¼ ì—…ë¡œë“œ")

    upload_col1, upload_col2 = st.columns([1.2, 2])
    new_file_processed = False

    with upload_col1:
        uploaded_files = st.file_uploader(
            "ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ê´€ë¦¬ .xlsx íŒŒì¼ ì—…ë¡œë“œ",
            type=["xlsx"],
            accept_multiple_files=True,
            help="ì˜ˆ: 2024ë…„ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ê´€ë¦¬.xlsx",
        )

        if uploaded_files:
            for f in uploaded_files:
                if f.name in st.session_state["processed_uploads"]:
                    continue
                try:
                    _, year, saved_path = loader.process_uploaded_energy_file(
                        file_obj=f,
                        original_filename=f.name,
                        base_dir=ENERGY_DIR,
                    )
                    st.session_state["processed_uploads"].add(f.name)
                    st.success(f"{f.name} ({year}) ì—…ë¡œë“œ ì™„ë£Œ")
                    new_file_processed = True
                except Exception as e:
                    st.error(f"{f.name} ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")

        if new_file_processed:
            st.rerun()

    with upload_col2:
        st.markdown("#### ì €ì¥ëœ íŒŒì¼ ëª©ë¡")
        df_all, files_meta, load_errors = load_all_energy_data()

        if files_meta:
            df_files = pd.DataFrame(files_meta).sort_values(
                ["ì—°ë„", "ì—…ë¡œë“œì‹œê°„"], ascending=[False, False]
            )
            st.table(df_files[["ì—°ë„", "íŒŒì¼ëª…", "ì—…ë¡œë“œì‹œê°„"]])
        else:
            st.info("ì €ì¥ëœ íŒŒì¼ ì—†ìŒ")

    st.markdown("---")

    if df_all is None:
        st.warning("ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # -----------------------------
    # KPIìš© ì˜¨ì‹¤ê°€ìŠ¤ ì§‘ê³„
    # -----------------------------
    datasets = analyzer.build_dashboard_datasets(df_all, baseline_map)

    # -----------------------------
    # ì—°ë„ í•„í„°
    # -----------------------------
    years = sorted(df_all["ì—°ë„"].unique().tolist())
    selected_year = max(years)
    selected_year = st.sidebar.selectbox("ì—°ë„ ì„ íƒ", years, index=years.index(selected_year))

    # ============================
    # ğŸ”¥ ì£¼ìš”ì§€í‘œ â€” ì—ë„ˆì§€ ë¶„ì„
    # ============================

    st.markdown("## ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ ì¶”ì´")

    raw_df = load_raw_year_data(selected_year)
    if raw_df is None:
        st.error(f"{selected_year}ë…„ ì›ë³¸ íŒŒì¼ ì—†ìŒ.")
        st.stop()

    org_col = raw_df.columns[2]
    U_col   = raw_df.columns[20]
    V_col   = raw_df.columns[21]
    W_col   = raw_df.columns[22]

    total_U = raw_df[U_col].sum(skipna=True)
    total_V = raw_df[V_col].sum(skipna=True)

    past_years = [selected_year-3, selected_year-2, selected_year-1]
    past_vals = []
    for y in past_years:
        df_past = load_raw_year_data(y)
        if df_past is not None:
            past_vals.append(df_past[df_past.columns[20]].sum(skipna=True))

    if len(past_vals) >= 1:
        past_avg = sum(past_vals)/len(past_vals)
        U_change_rate = (total_U - past_avg) / past_avg * 100 if past_avg else None
    else:
        past_avg = None
        U_change_rate = None

    k1, k2, k3 = st.columns(3)
    k1.metric("ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰(U í•©ê³„)", f"{total_U:,.0f}")
    k2.metric("ë©´ì ë‹¹ ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œëŸ‰(V í•©ê³„)", f"{total_V:,.0f}")
    k3.metric("3ê°œë…„ í‰ê·  ëŒ€ë¹„ ì¦ê°ë¥ ", "-" if U_change_rate is None else f"{U_change_rate:,.1f}%")

    # ============================
    # ì‹œì„¤êµ°ë³„(W) í‰ê· 
    # ============================

    st.markdown("### ì‹œì„¤êµ°ë³„ í‰ê·  ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰(Wì—´)")

    MEDICAL = ["ì¤‘ì•™ë³‘ì›","ë¶€ì‚°ë³‘ì›","ê´‘ì£¼ë³‘ì›","ëŒ€êµ¬ë³‘ì›","ëŒ€ì „ë³‘ì›","ì¸ì²œë³‘ì›"]
    WELFARE = ["ìˆ˜ì›ìš”ì–‘ì›","ê´‘ì£¼ìš”ì–‘ì›","ê¹€í•´ìš”ì–‘ì›","ëŒ€êµ¬ìš”ì–‘ì›","ëŒ€ì „ìš”ì–‘ì›","ë‚¨ì–‘ì£¼ìš”ì–‘ì›","ì›ì£¼ìš”ì–‘ì›","ì „ì£¼ìš”ì–‘ì›"]
    OTHER   = ["ë³¸ì‚¬","êµìœ¡ì—°êµ¬ì›","ë³´í›ˆì›","ì¬í™œì²´ìœ¡ì„¼í„°","íœ´ì–‘ì›"]

    def avg_group(names):
        return raw_df[raw_df[org_col].isin(names)][W_col].mean()

    g1,g2,g3 = st.columns(3)
    g1.metric("ì˜ë£Œì‹œì„¤ í‰ê· ", f"{avg_group(MEDICAL):,.1f}")
    g2.metric("ë³µì§€ì‹œì„¤ í‰ê· ", f"{avg_group(WELFARE):,.1f}")
    g3.metric("ê¸°íƒ€ì‹œì„¤ í‰ê· ", f"{avg_group(OTHER):,.1f}")

    # ============================
    # ì†Œì†ê¸°êµ¬ë³„ ë¶„ì„
    # ============================

    st.markdown("## ì†Œì†ê¸°êµ¬ë³„ ì—ë„ˆì§€ ì‚¬ìš© ë¶„ì„")

    df_group = raw_df.groupby(org_col).agg(
        Uí•©ê³„=(U_col,"sum"),
        Ví•©ê³„=(V_col,"sum"),
        Wí‰ê· =(W_col,"mean"),
    ).reset_index().rename(columns={org_col:"ê¸°ê´€ëª…"})

    def facility_type(name):
        if name in MEDICAL: return "ì˜ë£Œì‹œì„¤"
        if name in WELFARE: return "ë³µì§€ì‹œì„¤"
        if name in OTHER:   return "ê¸°íƒ€ì‹œì„¤"
        return "ê¸°íƒ€ì‹œì„¤"

    df_group["ì‹œì„¤êµ¬ë¶„"] = df_group["ê¸°ê´€ëª…"].apply(facility_type)
    df_group["ë¶„í¬ë¹„ìœ¨"] = df_group["Uí•©ê³„"] / total_U * 100 if total_U else None

    med_avg = avg_group(MEDICAL)
    wel_avg = avg_group(WELFARE)
    oth_avg = avg_group(OTHER)

    def avg_compare(row):
        if row["ì‹œì„¤êµ¬ë¶„"]=="ì˜ë£Œì‹œì„¤":
            return row["Wí‰ê· "]/med_avg if med_avg else None
        if row["ì‹œì„¤êµ¬ë¶„"]=="ë³µì§€ì‹œì„¤":
            return row["Wí‰ê· "]/wel_avg if wel_avg else None
        return row["Wí‰ê· "]/oth_avg if oth_avg else None

    df_group["í‰ê· ëŒ€ë¹„ì‚¬ìš©ë¹„ìœ¨"] = df_group.apply(avg_compare, axis=1)

    # 3ê°œë…„ ì¦ê°€ìœ¨
    def three_year_rate(name):
        past_vals = []
        for y in past_years:
            dfp = load_raw_year_data(y)
            if dfp is not None:
                val = dfp[dfp[dfp.columns[2]]==name][dfp.columns[20]].sum()
                past_vals.append(val)

        if len(past_vals)>=1:
            avg_p = sum(past_vals)/len(past_vals)
            now = df_group[df_group["ê¸°ê´€ëª…"]==name]["Uí•©ê³„"].iloc[0]
            if avg_p>0:
                return (now-avg_p)/avg_p*100
        return None

    df_group["3ê°œë…„ì¦ê°ë¥ "] = df_group["ê¸°ê´€ëª…"].apply(three_year_rate)

    # ê¸°ê´€ ìˆœì„œ ì ìš©
    ORDER = ["ë³¸ì‚¬","ì¤‘ì•™ë³‘ì›","ë¶€ì‚°ë³‘ì›","ê´‘ì£¼ë³‘ì›","ëŒ€êµ¬ë³‘ì›","ëŒ€ì „ë³‘ì›","ì¸ì²œë³‘ì›",
             "êµìœ¡ì—°êµ¬ì›","ë³´í›ˆì›","ìˆ˜ì›ìš”ì–‘ì›","ê´‘ì£¼ìš”ì–‘ì›","ê¹€í•´ìš”ì–‘ì›","ëŒ€êµ¬ìš”ì–‘ì›",
             "ëŒ€ì „ìš”ì–‘ì›","ë‚¨ì–‘ì£¼ìš”ì–‘ì›","ì›ì£¼ìš”ì–‘ì›","ì „ì£¼ìš”ì–‘ì›","ì¬í™œì²´ìœ¡ì„¼í„°","íœ´ì–‘ì›"]

    df_group["ê¸°ê´€ëª…"] = pd.Categorical(df_group["ê¸°ê´€ëª…"], categories=ORDER, ordered=True)
    df_group = df_group.sort_values("ê¸°ê´€ëª…")

    st.dataframe(df_group, use_container_width=True)

    # ============================
    # ì—ë„ˆì§€ ê¸°ë°˜ í”¼ë“œë°±
    # ============================

    st.markdown("## ì—ë„ˆì§€ ê¸°ë°˜ í”¼ë“œë°±")

    df_sel = df_all[df_all["ì—°ë„"]==selected_year]
    current_month = int(df_sel["ì›”"].max()) if not df_sel.empty else None

    baseline_val = baseline_map.get(selected_year)
    reduction_ratio = total_V / baseline_val * 100 if baseline_val else None

    f1,f2 = st.columns(2)
    f1.metric("í˜„ì¬ ì›”", f"{current_month}ì›”" if current_month else "-")
    f2.metric("ëª©í‘œë‹¬ì„± ê°ì¶•ë¥ (V/ê¸°ì¤€)", "-" if reduction_ratio is None else f"{reduction_ratio:,.1f}%")

    st.markdown("### ê¸°ê´€ë³„ í”¼ë“œë°±")

    df_fb = df_group.copy()
    df_fb["ì‚¬ìš©ë¶„í¬ìˆœìœ„"] = df_fb["Uí•©ê³„"].rank(ascending=False, method="dense")
    df_fb["3ê°œë…„ì¦ê°€ìˆœìœ„"] = df_fb["3ê°œë…„ì¦ê°ë¥ "].rank(ascending=False, method="dense")
    df_fb["í‰ê· ëŒ€ë¹„ìˆœìœ„"] = df_fb["í‰ê· ëŒ€ë¹„ì‚¬ìš©ë¹„ìœ¨"].rank(ascending=False, method="dense")

    if baseline_val:
        need = total_V - baseline_val
        need = need if need>0 else 0
        df_fb["ê¶Œì¥ê°ì¶•ëŸ‰"] = need * (df_fb["Uí•©ê³„"]/total_U)
    else:
        df_fb["ê¶Œì¥ê°ì¶•ëŸ‰"] = None

    def need_reason(row):
        if (row["3ê°œë…„ì¦ê°ë¥ "] is not None and row["3ê°œë…„ì¦ê°ë¥ "]>0) or \
           (row["í‰ê· ëŒ€ë¹„ì‚¬ìš©ë¹„ìœ¨"] is not None and row["í‰ê· ëŒ€ë¹„ì‚¬ìš©ë¹„ìœ¨"]>1):
            return "O"
        return "X"

    df_fb["ì¦ê°€ì‚¬ìœ ì œì¶œ"] = df_fb.apply(need_reason, axis=1)

    st.dataframe(df_fb, use_container_width=True)

    # ============================
    # ê¸°ì¡´ ìœ ì§€ êµ¬ê°„ â€” ê³µë‹¨ ì „ì²´ ë¶„ì„Â·ì½”ë©˜íŠ¸
    # ============================

    st.markdown("## ê³µë‹¨ ì „ì²´ ë¶„ì„Â·ì½”ë©˜íŠ¸")

    annual_total = analyzer.get_annual_ghg(df_all, by_agency=False)
    actual_emission = annual_total.query("ì—°ë„==@selected_year")["ì—°ê°„ ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œëŸ‰"].sum()

    recent_total_df, _ = analyzer.get_recent_years_ghg(annual_total, base_year=selected_year)

    fb_text = feedback.generate_overall_feedback(
        year=selected_year,
        actual_emission=actual_emission,
        baseline_emission=baseline_val,
        reduction_rate_pct=None,
        ratio_to_baseline=None,
        recent_total_df=recent_total_df,
        current_month=current_month,
    )
    st.write(fb_text)

# ============================================================
# 2) ê¸°ì¤€ë°°ì¶œëŸ‰ ê´€ë¦¬ íƒ­
# ============================================================

with tab_baseline:
    st.header("ê¸°ì¤€ë°°ì¶œëŸ‰ ê´€ë¦¬")

    st.markdown("### í˜„ì¬ ê¸°ì¤€ë°°ì¶œëŸ‰ ëª©ë¡")
    df_b = pd.DataFrame(baseline_records)
    if not df_b.empty:
        st.table(df_b)
    else:
        st.info("ë“±ë¡ëœ ê¸°ì¤€ë°°ì¶œëŸ‰ ì—†ìŒ")

    st.markdown("### ê¸°ì¤€ë°°ì¶œëŸ‰ ì‹ ê·œ ë“±ë¡")
    col1, col2, col3 = st.columns(3)

    with col1:
        new_year = st.number_input("ì—°ë„", min_value=2000, max_value=2100, step=1)

    with col2:
        new_val = st.number_input("ê¸°ì¤€ë°°ì¶œëŸ‰(tCO2eq)", min_value=0.0, step=100.0)

    if st.button("ì €ì¥"):
        baseline_mod.update_baseline_record(BASELINE_PATH, new_year, new_val)
        st.success("ê¸°ì¤€ë°°ì¶œëŸ‰ ì €ì¥ ì™„ë£Œ")
        st.rerun()

# ============================================================
# 3) ë””ë²„ê·¸/ì§„ë‹¨ íƒ­ (ì§„ë‹¨ ì½”ë“œ í¬í•¨)
# ============================================================

with tab_debug:

    st.header("ë””ë²„ê·¸ / êµ¬ì¡° ì§„ë‹¨")

    st.markdown("### loader.py êµ¬ì¡° ì§„ë‹¨")

    uploaded_debug_file = st.file_uploader("Excel êµ¬ì¡° ì§„ë‹¨ìš© íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx"])

    if uploaded_debug_file:
        from tempfile import NamedTemporaryFile
        with NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
            tmp.write(uploaded_debug_file.read())
            tmp_path = Path(tmp.name)

        try:
            res = loader.validate_excel_file(tmp_path)
            st.json(res)
        except Exception as e:
            st.error(f"ì§„ë‹¨ ì˜¤ë¥˜: {e}")

    st.markdown("---")

    # ============================================================
    # ğŸ” ì‹¤í–‰ í™˜ê²½ ì§„ë‹¨ ì½”ë“œ (loader.py í™•ì¸)
    # ============================================================

    with st.expander("ğŸ§ª ì‹¤í–‰ í™˜ê²½ ì§„ë‹¨: loader.py í™•ì¸"):
        import modules.loader as ld
        import inspect

        st.subheader("ğŸ“Œ í˜„ì¬ Streamlitì´ ì‚¬ìš© ì¤‘ì¸ loader.py íŒŒì¼ ê²½ë¡œ")
        st.code(ld.__file__)

        st.subheader("ğŸ“Œ loader.py í•¨ìˆ˜ ëª©ë¡")
        st.write(dir(ld))

        st.subheader("ğŸ“Œ loader.py ì‹¤ì œ ì†ŒìŠ¤ ì½”ë“œ")
        try:
            st.code(inspect.getsource(ld), language="python")
        except:
            st.error("loader.py ì†ŒìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
